{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trainbtp.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \\nWith apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this week–FBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clinton’s email server, the ranking Democrats on the relevant committees didn’t hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \\nAs we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \\n— Jason Chaffetz (@jasoninthehouse) October 28, 2016 \\nOf course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of “an unrelated case”–which we now know to be Anthony Weiner’s sexting with a teenager. But apparently such little things as facts didn’t matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary wins–at least two years’ worth, and possibly an entire term’s worth of them. Apparently Chaffetz thought the FBI was already doing his work for him–resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \\nBut according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetz’ sins. That aide told Shareblue that his boss and other Democrats didn’t even know about Comey’s letter at the time–and only found out when they checked Twitter. “Democratic Ranking Members on the relevant committees didn’t receive Comey’s letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didn’ receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.” \\nSo let’s see if we’ve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \\nThere has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \\nWhat it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didn’t even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesn’t trample on basic standards of fairness, I don’t know what does. \\nGranted, it’s not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetz’ planned fishing expedition. But that doesn’t mean we can’t turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \\nDarrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right\\'s worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop(columns=['id', 'title', 'author'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20761"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        house dem aide: we didn’t even see comey’s let...\n",
       "1        ever get the feeling your life circles the rou...\n",
       "2        why the truth might get you fired october 29, ...\n",
       "3        videos 15 civilians killed in single us airstr...\n",
       "4        print \\nan iranian woman has been sentenced to...\n",
       "                               ...                        \n",
       "20795    rapper t. i. unloaded on black celebrities who...\n",
       "20796    when the green bay packers lost to the washing...\n",
       "20797    the macy’s of today grew from the union of sev...\n",
       "20798    nato, russia to hold parallel exercises in bal...\n",
       "20799      david swanson is an author, activist, journa...\n",
       "Name: clean_news, Length: 20761, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_news'] = df['text'].str.lower()\n",
    "df['clean_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        house dem aide: we didn’t even see comey’s let...\n",
       "1        ever get the feeling your life circles the rou...\n",
       "2        why the truth might get you fired october 29, ...\n",
       "3        videos 15 civilians killed in single us airstr...\n",
       "4        print an iranian woman has been sentenced to s...\n",
       "                               ...                        \n",
       "20795    rapper t. i. unloaded on black celebrities who...\n",
       "20796    when the green bay packers lost to the washing...\n",
       "20797    the macy’s of today grew from the union of sev...\n",
       "20798    nato, russia to hold parallel exercises in bal...\n",
       "20799      david swanson is an author, activist, journa...\n",
       "Name: clean_news, Length: 20761, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_news'] = df['clean_news'].str.replace('[^A-Za-z0-9\\s]', '')\n",
    "df['clean_news'] = df['clean_news'].str.replace('\\n', '')\n",
    "df['clean_news'] = df['clean_news'].str.replace('\\s+', ' ')\n",
    "df['clean_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>house dem aide: didn’t even see comey’s letter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>truth might get fired october 29, 2016 tension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>videos 15 civilians killed single us airstrike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                          clean_news  \n",
       "0  house dem aide: didn’t even see comey’s letter...  \n",
       "1  ever get feeling life circles roundabout rathe...  \n",
       "2  truth might get fired october 29, 2016 tension...  \n",
       "3  videos 15 civilians killed single us airstrike...  \n",
       "4  print iranian woman sentenced six years prison...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['clean_news'] = df['clean_news'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239494"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_news'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding data\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_news'])\n",
    "padded_seq = pad_sequences(sequences, maxlen=500, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding index\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13342001,  0.19998001, -0.25533   , -0.051146  ,  0.11102   ,\n",
       "        0.31290999,  0.20461001, -0.16978   ,  0.23591   , -0.19870999,\n",
       "        0.27361   , -0.44758999,  0.15266   ,  0.36144999,  0.042599  ,\n",
       "       -0.375     ,  0.35876   ,  0.38881999, -0.71937001,  0.45868   ,\n",
       "       -0.026418  , -0.28536001, -0.30691999, -0.27586001,  0.19926   ,\n",
       "        0.36140999,  0.11371   , -0.34173   ,  0.44716999,  0.027599  ,\n",
       "       -0.12358   ,  0.43586001, -0.043002  ,  0.019661  ,  0.0075122 ,\n",
       "       -0.092998  , -0.14560001,  0.21397001,  0.043995  , -0.78390002,\n",
       "       -0.082397  , -0.27105001, -0.29763001,  0.18043999, -0.18894   ,\n",
       "       -0.75373   , -0.23617999,  0.019604  , -0.38022   , -0.10602   ,\n",
       "       -0.031779  ,  0.37211999,  0.20428   ,  1.02499998, -0.24906   ,\n",
       "       -2.09050012,  0.12701   ,  0.043019  ,  0.99423999,  0.025519  ,\n",
       "       -0.12768   ,  0.68374002, -0.45173001, -0.36655   ,  1.12810004,\n",
       "       -0.16283999,  0.18392999, -0.32929999,  0.73676997, -0.30932   ,\n",
       "        0.11571   ,  0.42056   ,  0.24158999, -0.034754  ,  0.26993999,\n",
       "        0.079367  ,  0.10831   ,  0.18488   , -0.51532   , -0.50993001,\n",
       "       -0.21547   , -0.085151  , -0.47334999, -0.26287001, -1.08969998,\n",
       "        0.41223001, -0.68421   , -1.05700004,  0.43463999,  0.036954  ,\n",
       "       -0.34614   , -0.72139001,  0.34325001,  0.66551   ,  0.18935999,\n",
       "        0.34676999, -0.58973998, -0.74396998,  0.31310001,  0.03746   ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   263,     33,   1664,     98,   5161,  29152,    455,   2398,\n",
       "         2133,    400,    585,   1649,   5599,     34,     13,    851,\n",
       "          195,    401,   2018,  18232,    860,  21158,   3122,   2786,\n",
       "          692,    794,    274,   2102,   3296,  11420,    141,     43,\n",
       "         2279,  92511,   3242,   3788,   3712,   3296,    857,    174,\n",
       "         3592,     34,  10160,  58240,   3369,     20,    233,     23,\n",
       "         3345,  21158,  10415,    618,     12,    526,   2537,   2552,\n",
       "         8371,    512,    743,   1539,    668,    157,     16,  23486,\n",
       "          955,  25165,    354,     10,    178,   3833,     60,  11867,\n",
       "          931,   2822,     19,    890,   5150,    190,     53,   5617,\n",
       "         2279,   4668,     46,    275,    857,    224,     15,      4,\n",
       "         2267,     15,      1,   2279,  10160,     67,   8823,  11583,\n",
       "       125498,  21595,   3905, 125499,  92512, 125500,   5696,  33536,\n",
       "        28289,   8340,  19700,  75640,   4604,      4,    176,    278,\n",
       "        24578,   4604,    177,    388,    957,   1691,    461,   1381,\n",
       "           23,    372,     40,   2040,     45,     36,    430,     68,\n",
       "         8760,   3767,  19364,    191,   1437,    802,  58241,   2514,\n",
       "         8128,   1688,   2660,    165,     13,    509,  10561,  12804,\n",
       "        18498,   9015,   6776,    113,  92513,  58242,  10638,  14236,\n",
       "        75641,    343,      9,    137,    678,    539,    403,    107,\n",
       "        58243,   2582,  20370,  32286,      1,  10160,   7366,   3739,\n",
       "         8371,    502,      6,    877,    184,    618,  14691,    457,\n",
       "           58,    133,    563,    191,      2,  92514,   1218,    493,\n",
       "         1159,   1218,     15,     43,    405,    210,    493,   1218,\n",
       "         8213,    479,     58,     32,     58,   1664,  32287,   1812,\n",
       "          574,     19,   2771,  21596,      1,  32286,  65316,   1051,\n",
       "        12390,    301,     60,    560,   1051,     19,    636,      8,\n",
       "        21596,    716,  14092,  38313,   1766,   1968,   3826,   2294,\n",
       "          292,     13,    116,  21596,   1182,    315,   8269,  13681,\n",
       "         1249,  20026,     59,  15509,   8518,     24,      8,      4,\n",
       "            5,  20746,   3369,     79,   3154,     79,    320,    968,\n",
       "         2144,  20027,     40,      4,   1701,    266,    491,   2965,\n",
       "        15509,     13,    604, 125501,    525,   7621,  75642,      1,\n",
       "        40288,      1,  22970,   1662,      1,    495,   1301,    648,\n",
       "          491,      1,    731,   1548,     24,      8,   2555,   1488,\n",
       "          648,   1983,    216,  12391,    154,    224,     48,     64,\n",
       "            8,    113,  15510,   1662,    246,    320,    133,   1035,\n",
       "         1086,    149,    214,    688,      1,   1043,   1816,  19365,\n",
       "            1,     10,     18,      8,   4909,   3850,     34,     13,\n",
       "        10416,    965,     90,   2865,   1596,    203,    190,  13815,\n",
       "         1523,  21158,    526,    107,     12,    178,   2654,     13,\n",
       "         9433,     80,    162,    658,   1799,   2845,    283,  28290,\n",
       "        45586,   4311,     21,  21158,  14237,   5090,    136,     43,\n",
       "           49,   3304,   9015,   2268,     90,    360,   2606,    488,\n",
       "         1136,    361,   1390,   1362,    488,   1136,    260,    952,\n",
       "         2014,    361,      1,    192,    195,    401,   2018,   4486,\n",
       "          401,     51,     47,   7932,    537,   1546,     19,    361,\n",
       "         3827,   1651,    647,    798,   1089,      1,    184,    991,\n",
       "           35,   3462,    689,     12,    401,   6011,   8270,  75643,\n",
       "        34935,    163,  58244,   7395,   1488,      4,  12078,    680,\n",
       "          362,  15711,    462,   1701,     40,     55,     10,   1900,\n",
       "        52877,  12280,   3712,     55,  20747,   1082,  13544,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_seq, df['label'], test_size=0.05, random_state=42, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout, Dense, Embedding\n",
    "from keras import Sequential\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(vocab_size+1, 100, weights=[embedding_matrix], trainable=False),\n",
    "#     Dropout(0.2),\n",
    "#     LSTM(128, return_sequences=True),\n",
    "#     LSTM(128),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(512),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(256),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 100, weights=[embedding_matrix], trainable=False),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(256),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         23949500  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 100)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24100029 (91.93 MB)\n",
      "Trainable params: 150529 (588.00 KB)\n",
      "Non-trainable params: 23949500 (91.36 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Trains the model for a fixed number of epochs (dataset iterations).\n",
       "\n",
       "Args:\n",
       "    x: Input data. It could be:\n",
       "      - A Numpy array (or array-like), or a list of arrays\n",
       "        (in case the model has multiple inputs).\n",
       "      - A TensorFlow tensor, or a list of tensors\n",
       "        (in case the model has multiple inputs).\n",
       "      - A dict mapping input names to the corresponding array/tensors,\n",
       "        if the model has named inputs.\n",
       "      - A `tf.data` dataset. Should return a tuple\n",
       "        of either `(inputs, targets)` or\n",
       "        `(inputs, targets, sample_weights)`.\n",
       "      - A generator or `keras.utils.Sequence` returning `(inputs,\n",
       "        targets)` or `(inputs, targets, sample_weights)`.\n",
       "      - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
       "        callable that takes a single argument of type\n",
       "        `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
       "        `DatasetCreator` should be used when users prefer to specify the\n",
       "        per-replica batching and sharding logic for the `Dataset`.\n",
       "        See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
       "        information.\n",
       "      A more detailed description of unpacking behavior for iterator\n",
       "      types (Dataset, generator, Sequence) is given below. If these\n",
       "      include `sample_weights` as a third component, note that sample\n",
       "      weighting applies to the `weighted_metrics` argument but not the\n",
       "      `metrics` argument in `compile()`. If using\n",
       "      `tf.distribute.experimental.ParameterServerStrategy`, only\n",
       "      `DatasetCreator` type is supported for `x`.\n",
       "    y: Target data. Like the input data `x`,\n",
       "      it could be either Numpy array(s) or TensorFlow tensor(s).\n",
       "      It should be consistent with `x` (you cannot have Numpy inputs and\n",
       "      tensor targets, or inversely). If `x` is a dataset, generator,\n",
       "      or `keras.utils.Sequence` instance, `y` should\n",
       "      not be specified (since targets will be obtained from `x`).\n",
       "    batch_size: Integer or `None`.\n",
       "        Number of samples per gradient update.\n",
       "        If unspecified, `batch_size` will default to 32.\n",
       "        Do not specify the `batch_size` if your data is in the\n",
       "        form of datasets, generators, or `keras.utils.Sequence`\n",
       "        instances (since they generate batches).\n",
       "    epochs: Integer. Number of epochs to train the model.\n",
       "        An epoch is an iteration over the entire `x` and `y`\n",
       "        data provided\n",
       "        (unless the `steps_per_epoch` flag is set to\n",
       "        something other than None).\n",
       "        Note that in conjunction with `initial_epoch`,\n",
       "        `epochs` is to be understood as \"final epoch\".\n",
       "        The model is not trained for a number of iterations\n",
       "        given by `epochs`, but merely until the epoch\n",
       "        of index `epochs` is reached.\n",
       "    verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
       "        0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
       "        'auto' becomes 1 for most cases, but 2 when used with\n",
       "        `ParameterServerStrategy`. Note that the progress bar is not\n",
       "        particularly useful when logged to a file, so verbose=2 is\n",
       "        recommended when not running interactively (eg, in a production\n",
       "        environment). Defaults to 'auto'.\n",
       "    callbacks: List of `keras.callbacks.Callback` instances.\n",
       "        List of callbacks to apply during training.\n",
       "        See `tf.keras.callbacks`. Note\n",
       "        `tf.keras.callbacks.ProgbarLogger` and\n",
       "        `tf.keras.callbacks.History` callbacks are created automatically\n",
       "        and need not be passed into `model.fit`.\n",
       "        `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
       "        `verbose` argument to `model.fit`.\n",
       "        Callbacks with batch-level calls are currently unsupported with\n",
       "        `tf.distribute.experimental.ParameterServerStrategy`, and users\n",
       "        are advised to implement epoch-level calls instead with an\n",
       "        appropriate `steps_per_epoch` value.\n",
       "    validation_split: Float between 0 and 1.\n",
       "        Fraction of the training data to be used as validation data.\n",
       "        The model will set apart this fraction of the training data,\n",
       "        will not train on it, and will evaluate\n",
       "        the loss and any model metrics\n",
       "        on this data at the end of each epoch.\n",
       "        The validation data is selected from the last samples\n",
       "        in the `x` and `y` data provided, before shuffling. This\n",
       "        argument is not supported when `x` is a dataset, generator or\n",
       "        `keras.utils.Sequence` instance.\n",
       "        If both `validation_data` and `validation_split` are provided,\n",
       "        `validation_data` will override `validation_split`.\n",
       "        `validation_split` is not yet supported with\n",
       "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
       "    validation_data: Data on which to evaluate\n",
       "        the loss and any model metrics at the end of each epoch.\n",
       "        The model will not be trained on this data. Thus, note the fact\n",
       "        that the validation loss of data provided using\n",
       "        `validation_split` or `validation_data` is not affected by\n",
       "        regularization layers like noise and dropout.\n",
       "        `validation_data` will override `validation_split`.\n",
       "        `validation_data` could be:\n",
       "          - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
       "          - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
       "            arrays.\n",
       "          - A `tf.data.Dataset`.\n",
       "          - A Python generator or `keras.utils.Sequence` returning\n",
       "          `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
       "        `validation_data` is not yet supported with\n",
       "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
       "    shuffle: Boolean (whether to shuffle the training data\n",
       "        before each epoch) or str (for 'batch'). This argument is\n",
       "        ignored when `x` is a generator or an object of tf.data.Dataset.\n",
       "        'batch' is a special option for dealing\n",
       "        with the limitations of HDF5 data; it shuffles in batch-sized\n",
       "        chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
       "    class_weight: Optional dictionary mapping class indices (integers)\n",
       "        to a weight (float) value, used for weighting the loss function\n",
       "        (during training only).\n",
       "        This can be useful to tell the model to\n",
       "        \"pay more attention\" to samples from\n",
       "        an under-represented class. When `class_weight` is specified\n",
       "        and targets have a rank of 2 or greater, either `y` must be\n",
       "        one-hot encoded, or an explicit final dimension of `1` must\n",
       "        be included for sparse class labels.\n",
       "    sample_weight: Optional Numpy array of weights for\n",
       "        the training samples, used for weighting the loss function\n",
       "        (during training only). You can either pass a flat (1D)\n",
       "        Numpy array with the same length as the input samples\n",
       "        (1:1 mapping between weights and samples),\n",
       "        or in the case of temporal data,\n",
       "        you can pass a 2D array with shape\n",
       "        `(samples, sequence_length)`,\n",
       "        to apply a different weight to every timestep of every sample.\n",
       "        This argument is not supported when `x` is a dataset, generator,\n",
       "        or `keras.utils.Sequence` instance, instead provide the\n",
       "        sample_weights as the third element of `x`.\n",
       "        Note that sample weighting does not apply to metrics specified\n",
       "        via the `metrics` argument in `compile()`. To apply sample\n",
       "        weighting to your metrics, you can specify them via the\n",
       "        `weighted_metrics` in `compile()` instead.\n",
       "    initial_epoch: Integer.\n",
       "        Epoch at which to start training\n",
       "        (useful for resuming a previous training run).\n",
       "    steps_per_epoch: Integer or `None`.\n",
       "        Total number of steps (batches of samples)\n",
       "        before declaring one epoch finished and starting the\n",
       "        next epoch. When training with input tensors such as\n",
       "        TensorFlow data tensors, the default `None` is equal to\n",
       "        the number of samples in your dataset divided by\n",
       "        the batch size, or 1 if that cannot be determined. If x is a\n",
       "        `tf.data` dataset, and 'steps_per_epoch'\n",
       "        is None, the epoch will run until the input dataset is\n",
       "        exhausted.  When passing an infinitely repeating dataset, you\n",
       "        must specify the `steps_per_epoch` argument. If\n",
       "        `steps_per_epoch=-1` the training will run indefinitely with an\n",
       "        infinitely repeating dataset.  This argument is not supported\n",
       "        with array inputs.\n",
       "        When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
       "          * `steps_per_epoch=None` is not supported.\n",
       "    validation_steps: Only relevant if `validation_data` is provided and\n",
       "        is a `tf.data` dataset. Total number of steps (batches of\n",
       "        samples) to draw before stopping when performing validation\n",
       "        at the end of every epoch. If 'validation_steps' is None,\n",
       "        validation will run until the `validation_data` dataset is\n",
       "        exhausted. In the case of an infinitely repeated dataset, it\n",
       "        will run into an infinite loop. If 'validation_steps' is\n",
       "        specified and only part of the dataset will be consumed, the\n",
       "        evaluation will start from the beginning of the dataset at each\n",
       "        epoch. This ensures that the same validation samples are used\n",
       "        every time.\n",
       "    validation_batch_size: Integer or `None`.\n",
       "        Number of samples per validation batch.\n",
       "        If unspecified, will default to `batch_size`.\n",
       "        Do not specify the `validation_batch_size` if your data is in\n",
       "        the form of datasets, generators, or `keras.utils.Sequence`\n",
       "        instances (since they generate batches).\n",
       "    validation_freq: Only relevant if validation data is provided.\n",
       "      Integer or `collections.abc.Container` instance (e.g. list, tuple,\n",
       "      etc.).  If an integer, specifies how many training epochs to run\n",
       "      before a new validation run is performed, e.g. `validation_freq=2`\n",
       "      runs validation every 2 epochs. If a Container, specifies the\n",
       "      epochs on which to run validation, e.g.\n",
       "      `validation_freq=[1, 2, 10]` runs validation at the end of the\n",
       "      1st, 2nd, and 10th epochs.\n",
       "    max_queue_size: Integer. Used for generator or\n",
       "      `keras.utils.Sequence` input only. Maximum size for the generator\n",
       "      queue.  If unspecified, `max_queue_size` will default to 10.\n",
       "    workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
       "        only. Maximum number of processes to spin up\n",
       "        when using process-based threading. If unspecified, `workers`\n",
       "        will default to 1.\n",
       "    use_multiprocessing: Boolean. Used for generator or\n",
       "        `keras.utils.Sequence` input only. If `True`, use process-based\n",
       "        threading. If unspecified, `use_multiprocessing` will default to\n",
       "        `False`. Note that because this implementation relies on\n",
       "        multiprocessing, you should not pass non-pickleable arguments to\n",
       "        the generator as they can't be passed easily to children\n",
       "        processes.\n",
       "\n",
       "Unpacking behavior for iterator-like inputs:\n",
       "    A common pattern is to pass a tf.data.Dataset, generator, or\n",
       "  tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
       "  yield not only features (x) but optionally targets (y) and sample\n",
       "  weights.  Keras requires that the output of such iterator-likes be\n",
       "  unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n",
       "  where the optional second and third elements will be used for y and\n",
       "  sample_weight respectively. Any other type provided will be wrapped in\n",
       "  a length one tuple, effectively treating everything as 'x'. When\n",
       "  yielding dicts, they should still adhere to the top-level tuple\n",
       "  structure.\n",
       "  e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
       "  features, targets, and weights from the keys of a single dict.\n",
       "    A notable unsupported data type is the namedtuple. The reason is\n",
       "  that it behaves like both an ordered datatype (tuple) and a mapping\n",
       "  datatype (dict). So given a namedtuple of the form:\n",
       "      `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
       "  it is ambiguous whether to reverse the order of the elements when\n",
       "  interpreting the value. Even worse is a tuple of the form:\n",
       "      `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
       "  where it is unclear if the tuple was intended to be unpacked into x,\n",
       "  y, and sample_weight or passed through as a single element to `x`. As\n",
       "  a result the data processing code will simply raise a ValueError if it\n",
       "  encounters a namedtuple. (Along with instructions to remedy the\n",
       "  issue.)\n",
       "\n",
       "Returns:\n",
       "    A `History` object. Its `History.history` attribute is\n",
       "    a record of training loss values and metrics values\n",
       "    at successive epochs, as well as validation loss values\n",
       "    and validation metrics values (if applicable).\n",
       "\n",
       "Raises:\n",
       "    RuntimeError: 1. If the model was never compiled or,\n",
       "    2. If `model.fit` is  wrapped in `tf.function`.\n",
       "\n",
       "    ValueError: In case of mismatch between the provided input data\n",
       "        and what the model expects or when the input data is empty.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\keras\\src\\engine\\training.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 65/198 [========>.....................] - ETA: 3:06 - loss: 0.6493 - accuracy: 0.6218"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size = 100,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the results\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
